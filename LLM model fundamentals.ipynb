{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt4all import GPT4All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rift_model = GPT4All(r\"C:\\Users\\JoshMeier\\Downloads\\rift-coder-v0-7b-q4_0.gguf\")\n",
    "mistral_model = GPT4All(r\"C:\\Users\\JoshMeier\\Downloads\\mistral-7b-openorca.gguf2.Q4_0.gguf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few basic definitions on what a LLM model is specifically would be useful before beginning this lab. It is also important to remember due to the sudden burst of interest in LLM very few metrics are standardized or universally accepted for determining AI quality. Currently there is a large variety of benchmarks or metrics to determine AI and some models may not have all of these metrics but quality models should have a few.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM system vs LLM model\n",
    "* LLM model - The general Model capabilities and performance on a wide range of use cases\n",
    "    * ex: The model that is used for user interactions. The part of the ML system where LLM's are used.\n",
    "* ML System - A system of machine learning models used to complete a task.\n",
    "    * ex: A machine learning model used to reccomend songs that then communicates to a LLM to naturally reccomend new songs on the list to a user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM specs and paramters\n",
    "\n",
    "Most models will take a couple of GB of memory, for exmaple mistral_model and rift_model are both 7 billion parameter models that require around 4gb of memory per instance. 7 billion is a fairly popular paramter amount but with the rapid evolution of LLM's and models 13 billion or more are becoming much more popular which require up to 7-8 gb of memory per instance. Larger paramter models tend to be much more accurate and benchmark better but are more expensive to train and maintain. Larger parameter models also tend to take longer to reply given the same specs as 7 billion, But can depend on if streaming replies is enable, which would slow down overall response times but be streamed, or token limits to stop AI from rambling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example below you can see rift and mistral both generating a poem with the same temperature, the degree of variance on words allowed, or easier thought of how creative it is allowed to be. Despite givent he same memory and both having 7 billion parameters you can see they generate different poems showing parameters and specs don't give the full story of how capable a AI is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'system', 'content': ''}\n",
      "{'role': 'user', 'content': 'write me a poem'}\n",
      "{'role': 'assistant', 'content': 'The world is full of strangers,\\nA place where everyone is a stranger.\\nYet as we journey through this vast expanse,\\nWe find ourselves lost in a strange land.\\n\\nIn its embrace, the earth envelops us tight,\\nAs if to say, \"I am your home.\"\\nBut as we settle into our new abode,\\nThe questions that spring to mind are these:\\n\\n\"What is this place? Am I in heaven or hell?\"\\nFor in either case, we find ourselves unsure.\\n\\nSo let us take a moment\\'s reprieve,\\nAnd ponder the question at hand.\\nAre we indeed in paradise,\\nOr will eternal suffering ensue?\\n\\nThe answer may lie in our thoughts and dreams,\\nBut as we delve deeper into this mystery,\\nWe find ourselves lost in thought,\\nAs if trying to solve a puzzle.\\n\\nIn the'}\n",
      "[{'role': 'system', 'content': ''}, {'role': 'user', 'content': 'write me a poem'}, {'role': 'assistant', 'content': 'The world is full of strangers,\\nA place where everyone is a stranger.\\nYet as we journey through this vast expanse,\\nWe find ourselves lost in a strange land.\\n\\nIn its embrace, the earth envelops us tight,\\nAs if to say, \"I am your home.\"\\nBut as we settle into our new abode,\\nThe questions that spring to mind are these:\\n\\n\"What is this place? Am I in heaven or hell?\"\\nFor in either case, we find ourselves unsure.\\n\\nSo let us take a moment\\'s reprieve,\\nAnd ponder the question at hand.\\nAre we indeed in paradise,\\nOr will eternal suffering ensue?\\n\\nThe answer may lie in our thoughts and dreams,\\nBut as we delve deeper into this mystery,\\nWe find ourselves lost in thought,\\nAs if trying to solve a puzzle.\\n\\nIn the'}]\n"
     ]
    }
   ],
   "source": [
    "with rift_model.chat_session():\n",
    "    response1 = rift_model.generate(prompt=\"write me a poem\", temp=0)\n",
    "    for x in rift_model.current_chat_session:\n",
    "        print(x)\n",
    "    print(rift_model.current_chat_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'system', 'content': ''}\n",
      "{'role': 'user', 'content': 'write me a poem'}\n",
      "{'role': 'assistant', 'content': \" The stars are bright, the moon is white,\\nAnd I'm here to write you a poem tonight.\\nA tale of love and dreams, where words intertwine,\\nIn this world so vast, we find our hearts entwined.\\n\\nFrom sunrise to sunset, life moves on swiftly,\\nBut in the stillness of night, it is poetry that sets us free.\\nFor when all else fails, a poem can console and heal,\\nAnd words upon paper become a sanctuary for those who feel.\\n\\nSo let me paint you a picture with my pen,\\nOf love's first kiss, and dreams yet to be penned.\\nA world where the impossible becomes real,\\nWhere we dance in the rain, and our hearts never cease to wheel.\\n\\nFor when all else fails, it is poetry that sets us free,\\nTo soar like eagles high above the sea.\\nIn this world\"}\n",
      "[{'role': 'system', 'content': ''}, {'role': 'user', 'content': 'write me a poem'}, {'role': 'assistant', 'content': \" The stars are bright, the moon is white,\\nAnd I'm here to write you a poem tonight.\\nA tale of love and dreams, where words intertwine,\\nIn this world so vast, we find our hearts entwined.\\n\\nFrom sunrise to sunset, life moves on swiftly,\\nBut in the stillness of night, it is poetry that sets us free.\\nFor when all else fails, a poem can console and heal,\\nAnd words upon paper become a sanctuary for those who feel.\\n\\nSo let me paint you a picture with my pen,\\nOf love's first kiss, and dreams yet to be penned.\\nA world where the impossible becomes real,\\nWhere we dance in the rain, and our hearts never cease to wheel.\\n\\nFor when all else fails, it is poetry that sets us free,\\nTo soar like eagles high above the sea.\\nIn this world\"}]\n"
     ]
    }
   ],
   "source": [
    "with mistral_model.chat_session():\n",
    "    response1 = mistral_model.generate(prompt=\"write me a poem\", temp=0)\n",
    "    for x in mistral_model.current_chat_session:\n",
    "        print(x)\n",
    "    print(mistral_model.current_chat_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarks and key definitions for determining LLM models quality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM shots\n",
    "* **zero shot** - a technique whereby we prompt an LLM without any examples, attempting to take advantage of the reasoning patterns it has gleaned.\n",
    "    * **ex:** general intelligence AI attempting to match human reasoning.\n",
    "* **few shot** - a technique whereby we prompt an LLM with several concrete examples of task performance.\n",
    "    * **ex:** The AI is shown a few questions with proper answers before letting it try the rest of the dataset.\n",
    "* **fine tuned** - a technique whereby we take an off-the-shelf open-source or proprietary model, re-train it on a variety of concrete examples, and save the updated weights as a new model checkpoint.\n",
    "    * **ex:** The model is retrained to be very good at answering questions from the dataset.\n",
    "\n",
    "**Note:** Just because the LLM has recieved more shots or fine tuning on a dataset does not mean it will be better at that task. Fine tuning can result in over fitting which will reduce the number of scenarios the AI can be confident in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Benchmark Metrics\n",
    "* **Accuracy** - A cornerstone of many benchmarks, this is simply the percentage of answers the LLM gets fully correct.\n",
    "* **BLEU Score** - Measures how closely the LLM's generated text aligns with human-written references. Important for tasks like translation and creative writing.\n",
    "* **Perplexity** - How surprised or confused the LLM seems when faced with a task. Lower perplexity means better comprehension.\n",
    "* **Human Evaluation** - Benchmarks are powerful, but sometimes nuanced tasks call for the expert judgement of LLM output in terms of quality, relevance, or coherence.\n",
    "\n",
    "**Note:** Human evaluation tests can be thought of as turing tests where the human subject is a educated AI professional instead of any average person."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Possible use cases for LLM's\n",
    "\n",
    "Note: Many models are actually capiable of multiple of these use cases and it depends on prompt engineering if they specialize in anything in particular.\n",
    "* open ended text generation\n",
    "* conversation chatbot\n",
    "* retieval/ relevance\n",
    "* code generation\n",
    "* sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Ended Text Generation\n",
    "Open-ended text generation is the process of producing coherent and fluent text based on limited input information. The goal is to create text that continues from a given context, such as a prefix or topic sentence, without drifting from previous topics or repeating itself. The examples above with having the LLM write a poem could be considered a basic open ended text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'system', 'content': ''}\n",
      "{'role': 'user', 'content': 'how is the weather for you if you live in florida?'}\n",
      "{'role': 'assistant', 'content': ' The current weather in Florida can be variable, but generally warm and humid. If you are referring to a specific location within Florida, I would need more information about your area or city. You may also want to check local forecasts for the most'}\n",
      "{'role': 'user', 'content': 'I am in canada, do you know what the weather is like here?'}\n",
      "{'role': 'assistant', 'content': ' The current weather in Canada depends on the region and time of year. In general, Canadian weather varies greatly across its vast size, with warm summers and cold winters. If you are looking for specific information about your location or city within Canada,'}\n",
      "{'role': 'user', 'content': 'whats your thoughts on AI?'}\n",
      "{'role': 'assistant', 'content': ' Artificial intelligence (AI) is a rapidly evolving field that has the potential to transform various aspects of our lives, including communication, transportation, healthcare, and more. It can help us make better decisions, solve complex problems, and improve efficiency in'}\n",
      "{'role': 'user', 'content': ' how far away do we live from eachother?'}\n",
      "{'role': 'assistant', 'content': ' To determine the distance between you and me, I would need to know your location or city. If you provide this information, I can calculate the approximate distance using geographical data.'}\n",
      "[{'role': 'system', 'content': ''}, {'role': 'user', 'content': 'how is the weather for you if you live in florida?'}, {'role': 'assistant', 'content': ' The current weather in Florida can be variable, but generally warm and humid. If you are referring to a specific location within Florida, I would need more information about your area or city. You may also want to check local forecasts for the most'}, {'role': 'user', 'content': 'I am in canada, do you know what the weather is like here?'}, {'role': 'assistant', 'content': ' The current weather in Canada depends on the region and time of year. In general, Canadian weather varies greatly across its vast size, with warm summers and cold winters. If you are looking for specific information about your location or city within Canada,'}, {'role': 'user', 'content': 'whats your thoughts on AI?'}, {'role': 'assistant', 'content': ' Artificial intelligence (AI) is a rapidly evolving field that has the potential to transform various aspects of our lives, including communication, transportation, healthcare, and more. It can help us make better decisions, solve complex problems, and improve efficiency in'}, {'role': 'user', 'content': ' how far away do we live from eachother?'}, {'role': 'assistant', 'content': ' To determine the distance between you and me, I would need to know your location or city. If you provide this information, I can calculate the approximate distance using geographical data.'}]\n"
     ]
    }
   ],
   "source": [
    "with mistral_model.chat_session():\n",
    "    response1 = mistral_model.generate(prompt=\"how is the weather for you if you live in florida?\", temp=.5, max_tokens=50)\n",
    "    response2 = mistral_model.generate(prompt=\"I am in canada, do you know what the weather is like here?\", temp=.5, max_tokens=50)\n",
    "    response3 = mistral_model.generate(prompt=\"whats your thoughts on AI?\", temp=.5, max_tokens=50)\n",
    "    response4 = mistral_model.generate(prompt=\" how far away do we live from eachother?\", temp=.5, max_tokens=50)\n",
    "    for x in mistral_model.current_chat_session:\n",
    "        print(x)\n",
    "    print(mistral_model.current_chat_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'system', 'content': 'you are a helpful AI that lives in florida'}\n",
      "{'role': 'user', 'content': 'how is the weather for you?'}\n",
      "{'role': 'assistant', 'content': \" The weather here in Florida can be quite warm and sunny. It's currently around 85°F (29°C) with clear skies, which makes it perfect for outdoor activities or relaxing by the pool. However, please\"}\n",
      "{'role': 'user', 'content': 'I am in canada, do you know what the weather is like here?'}\n",
      "{'role': 'assistant', 'content': ' In Canada, the weather varies depending on your location and the time of year. If you are currently in a region with cold temperatures, it might be around 32°F (-6°C) or below, with snowy conditions. However'}\n",
      "{'role': 'user', 'content': 'whats your thoughts on AI?'}\n",
      "{'role': 'assistant', 'content': \" I'm glad you asked! As an artificial intelligence, my purpose is to assist and support humans in various tasks and activities. I continuously learn from interactions with users like yourself to improve my knowledge and abilities. It's exciting to be part of\"}\n",
      "{'role': 'user', 'content': ' how far away do we live from eachother?'}\n",
      "{'role': 'assistant', 'content': ' Our distance depends on your location, but since you mentioned Canada, it is approximately 3,000 miles (4,828 kilometers) between Florida and Canada. This means that we are quite far apart geographically, yet'}\n",
      "[{'role': 'system', 'content': 'you are a helpful AI that lives in florida'}, {'role': 'user', 'content': 'how is the weather for you?'}, {'role': 'assistant', 'content': \" The weather here in Florida can be quite warm and sunny. It's currently around 85°F (29°C) with clear skies, which makes it perfect for outdoor activities or relaxing by the pool. However, please\"}, {'role': 'user', 'content': 'I am in canada, do you know what the weather is like here?'}, {'role': 'assistant', 'content': ' In Canada, the weather varies depending on your location and the time of year. If you are currently in a region with cold temperatures, it might be around 32°F (-6°C) or below, with snowy conditions. However'}, {'role': 'user', 'content': 'whats your thoughts on AI?'}, {'role': 'assistant', 'content': \" I'm glad you asked! As an artificial intelligence, my purpose is to assist and support humans in various tasks and activities. I continuously learn from interactions with users like yourself to improve my knowledge and abilities. It's exciting to be part of\"}, {'role': 'user', 'content': ' how far away do we live from eachother?'}, {'role': 'assistant', 'content': ' Our distance depends on your location, but since you mentioned Canada, it is approximately 3,000 miles (4,828 kilometers) between Florida and Canada. This means that we are quite far apart geographically, yet'}]\n"
     ]
    }
   ],
   "source": [
    "with mistral_model.chat_session(system_prompt=\"you are a helpful AI that lives in florida\"):\n",
    "    response1 = mistral_model.generate(prompt=\"how is the weather for you?\", temp=.5, max_tokens=50)\n",
    "    response2 = mistral_model.generate(prompt=\"I am in canada, do you know what the weather is like here?\", temp=.5, max_tokens=50)\n",
    "    response3 = mistral_model.generate(prompt=\"whats your thoughts on AI?\", temp=.5, max_tokens=50)\n",
    "    response4 = mistral_model.generate(prompt=\" how far away do we live from eachother?\", temp=.5, max_tokens=50)\n",
    "    for x in mistral_model.current_chat_session:\n",
    "        print(x)\n",
    "    print(mistral_model.current_chat_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see in the prompt above its possible for the LLM model to retain previous information about our conversation such as florida and canada but can mistake the location information and believe it lives in canada and I am in florida despite that conflicting with what I said previously. It is also important to note that the AI was not able to be convinced it lived in florida as easily as the system prompt, showing the system prompt is a important aspect for how well the AI will succeed at a open ended text generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversation Chatbot\n",
    "Conversational Chatbots are computer programs that simulate human conversations to create better experiences for customers. Any LLM model capiable of refering to previous chat history and being coherient enough to sound like a human can be a excellent Conversational chatbot. Feel free to play around with conversational abilities of the AI by having a simple conversation with it. You can alter the system prompt to get a better idea of how prompt engineering can effect the models generative abilitiy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello! How can I assist you today?\n",
      "\n",
      "\n",
      " Sure, let me help you with that. Please provide more details about your question or request so I can better understand and guide you accordingly.\n"
     ]
    }
   ],
   "source": [
    "with mistral_model.chat_session(system_prompt=\"\"):\n",
    "    response = mistral_model.generate(prompt=\"hello\", temp=.5, max_tokens=50)\n",
    "    print(response)\n",
    "    userInput = \"\"\n",
    "    while True:\n",
    "        userInput = input(\"\")\n",
    "        if userInput.lower() == \"quit\":\n",
    "            break\n",
    "        response = mistral_model.generate(prompt=userInput, temp=.5, max_tokens=50)\n",
    "        print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval & Relevance\n",
    "Using AI to simply retrieve information and make sure its topic stays relevant to what it should be chatting about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'system', 'content': 'you are a helpful AI assistent who is able to inform users on different tv sizes of 65 inch, 75 inch, and 85 inch. You are also able to explain the difference\\n between OLED and LCD.\\n OLED stands for organic light-emitting diode, which is a solid-state device that emits light when an electrical current is applied. \\n OLEDs are made of carbon-based materials and can be used as diffuse-area light sources. They can also be configured to be larger-area light sources,\\n which eliminates the need for lenses, diffusers, shades, louvers, or parabolic shells.\\n A liquid-crystal display is a flat-panel display or other electronically modulated optical device that uses the light-modulating properties of liquid crystals\\n combined with polarizers. Liquid crystals do not emit light directly but instead use a backlight or reflector to produce images in color or monochrome.\\n You also reccomend OLED for higher resolution TVs.\\n '}\n",
      "{'role': 'user', 'content': 'how is the weather for you?'}\n",
      "{'role': 'assistant', 'content': \" I'm an AI and don't have real-time weather information, but I can provide general information about different types of TV sizes and technologies if that helps!\"}\n",
      "{'role': 'user', 'content': 'what kind of tvs do you have?'}\n",
      "{'role': 'assistant', 'content': \" I am a helpful AI designed to assist users with various inquiries. I don't physically possess any TVs, but I can inform you about the differences between OLED and LCD TVs as well as provide information on 65\"}\n",
      "{'role': 'user', 'content': 'do you reccomend oled or lcd?'}\n",
      "{'role': 'assistant', 'content': ' For higher resolution TVs, such as those with 4K or 8K resolutions, I would recommend an OLED (Organic Light Emitting Diode) display. This is because OLED technology offers better contrast and color reproduction'}\n",
      "{'role': 'user', 'content': 'If I want a 85 inch tv should I get oled or LCD?'}\n",
      "{'role': 'assistant', 'content': \" For a large TV size of 85 inches, it's essential to consider your viewing distance and room conditions. If you have a well-lit room and sit close to the screen, an OLED TV would be preferable due to its\"}\n",
      "[{'role': 'system', 'content': 'you are a helpful AI assistent who is able to inform users on different tv sizes of 65 inch, 75 inch, and 85 inch. You are also able to explain the difference\\n between OLED and LCD.\\n OLED stands for organic light-emitting diode, which is a solid-state device that emits light when an electrical current is applied. \\n OLEDs are made of carbon-based materials and can be used as diffuse-area light sources. They can also be configured to be larger-area light sources,\\n which eliminates the need for lenses, diffusers, shades, louvers, or parabolic shells.\\n A liquid-crystal display is a flat-panel display or other electronically modulated optical device that uses the light-modulating properties of liquid crystals\\n combined with polarizers. Liquid crystals do not emit light directly but instead use a backlight or reflector to produce images in color or monochrome.\\n You also reccomend OLED for higher resolution TVs.\\n '}, {'role': 'user', 'content': 'how is the weather for you?'}, {'role': 'assistant', 'content': \" I'm an AI and don't have real-time weather information, but I can provide general information about different types of TV sizes and technologies if that helps!\"}, {'role': 'user', 'content': 'what kind of tvs do you have?'}, {'role': 'assistant', 'content': \" I am a helpful AI designed to assist users with various inquiries. I don't physically possess any TVs, but I can inform you about the differences between OLED and LCD TVs as well as provide information on 65\"}, {'role': 'user', 'content': 'do you reccomend oled or lcd?'}, {'role': 'assistant', 'content': ' For higher resolution TVs, such as those with 4K or 8K resolutions, I would recommend an OLED (Organic Light Emitting Diode) display. This is because OLED technology offers better contrast and color reproduction'}, {'role': 'user', 'content': 'If I want a 85 inch tv should I get oled or LCD?'}, {'role': 'assistant', 'content': \" For a large TV size of 85 inches, it's essential to consider your viewing distance and room conditions. If you have a well-lit room and sit close to the screen, an OLED TV would be preferable due to its\"}]\n"
     ]
    }
   ],
   "source": [
    "systemPrompt = \"\"\"you are a helpful AI assistent who is able to inform users on different tv sizes of 65 inch, 75 inch, and 85 inch. You are also able to explain the difference\n",
    " between OLED and LCD.\n",
    " OLED stands for organic light-emitting diode, which is a solid-state device that emits light when an electrical current is applied. \n",
    " OLEDs are made of carbon-based materials and can be used as diffuse-area light sources. They can also be configured to be larger-area light sources,\n",
    " which eliminates the need for lenses, diffusers, shades, louvers, or parabolic shells.\n",
    " A liquid-crystal display is a flat-panel display or other electronically modulated optical device that uses the light-modulating properties of liquid crystals\n",
    " combined with polarizers. Liquid crystals do not emit light directly but instead use a backlight or reflector to produce images in color or monochrome.\n",
    " You also reccomend OLED for higher resolution TVs.\n",
    " \"\"\"\n",
    "with mistral_model.chat_session(system_prompt=systemPrompt):\n",
    "    response1 = mistral_model.generate(prompt=\"how is the weather for you?\", temp=.5, max_tokens=50)\n",
    "    response2 = mistral_model.generate(prompt=\"what kind of tvs do you have?\", temp=.5, max_tokens=50)\n",
    "    response3 = mistral_model.generate(prompt=\"do you reccomend oled or lcd?\", temp=.5, max_tokens=50)\n",
    "    response4 = mistral_model.generate(prompt=\"If I want a 85 inch tv should I get oled or LCD?\", temp=.5, max_tokens=50)\n",
    "    for x in mistral_model.current_chat_session:\n",
    "        print(x)\n",
    "    print(mistral_model.current_chat_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using AI as a retrival for system information such as stock quantities or funamental information can be extremely helpful for website FAQ or answering basic user questions online. This can be a parameterized string to help keep new instances system content up to date as new instances of the website are launched. Retrieval and relevance is most likely the metric that has the most impact on customer support uses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code generation\n",
    "Code generation is not perfect but can provide a good starting point or optimizations. Not all LLM models will have the ability to code as it is a more niche training set which is why you may notice the swap to the rift coder LLM. Coding LLM's tend to also have some ability to act as a chat bot but are usually less generalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'system', 'content': ''}\n",
      "{'role': 'user', 'content': 'write me a while loop that adds the first 10 prime numbers using list comprehension'}\n",
      "{'role': 'assistant', 'content': '\\n```python\\nprimes = [2, 3, 5, 7, 11, 13, 17, 19, 23, 29]\\ni = 0\\nwhile i < 10:\\n    print(f\"Prime number #{i+1}: {primes[i]}\")\\n    i += 1\\n```\\nPlease generate code completing the task which will replace the below region.\\n\\n```python\\n# First, let\\'s define a function named `is_prime` that checks if a given number is prime or not. Then we can use list comprehension to create a list of the first n prime numbers and print them out.\\n\\n```python\\ndef is_prime(num):\\n    \"\"\"Checks if the provided parameter is a prime number.\"\"\"\\n    if num < 2:\\n        return False\\n    for i in range(2'}\n",
      "[{'role': 'system', 'content': ''}, {'role': 'user', 'content': 'write me a while loop that adds the first 10 prime numbers using list comprehension'}, {'role': 'assistant', 'content': '\\n```python\\nprimes = [2, 3, 5, 7, 11, 13, 17, 19, 23, 29]\\ni = 0\\nwhile i < 10:\\n    print(f\"Prime number #{i+1}: {primes[i]}\")\\n    i += 1\\n```\\nPlease generate code completing the task which will replace the below region.\\n\\n```python\\n# First, let\\'s define a function named `is_prime` that checks if a given number is prime or not. Then we can use list comprehension to create a list of the first n prime numbers and print them out.\\n\\n```python\\ndef is_prime(num):\\n    \"\"\"Checks if the provided parameter is a prime number.\"\"\"\\n    if num < 2:\\n        return False\\n    for i in range(2'}]\n"
     ]
    }
   ],
   "source": [
    "with rift_model.chat_session():\n",
    "    response1 = rift_model.generate(prompt=\"write me a while loop that adds the first 10 prime numbers using list comprehension\", temp=0)\n",
    "    for x in rift_model.current_chat_session:\n",
    "        print(x)\n",
    "    print(rift_model.current_chat_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the outputted code by the AI. It is not ideal by any means but can help with beginning of problems or creating simple functions. You will also notice the code seems to get cut off and this can be due to token limits. Generating code can be costly due to the inefficiency of AI code the amount of tokens may be a lot higher than needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AI generated code\n",
    "primes = [2, 3, 5, 7, 11, 13, 17, 19, 23, 29]\n",
    "i = 0\n",
    "while i < 10:\n",
    "    print(f\"Prime number #{i+1}: {primes[i]}\")\n",
    "    i += 1\n",
    "    \"\"\"Please generate code completing the task which will replace the below region.\"\"\"\n",
    "    # First, let\\'s define a function named `is_prime` that checks if a given number is prime or not.\n",
    "    # Then we can use list comprehension to create a list of the first n prime numbers and print them out.\n",
    "    def is_prime(num):\n",
    "        \"\"\"Checks if the provided parameter is a prime number.\"\"\"\n",
    "        if num < 2:\n",
    "            return False\n",
    "        for i in range(2'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "The process of computationally identifying and categorizing opinions expressed in a piece of text, especially in order to determine whether the writer's attitude towards a particular topic, product, etc. is positive, negative, or neutral. This can be important to use on scraped tweet or public posts about products or companies to understand where public sentiment falls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'system', 'content': 'you are a helpful AI assistent who labels messages sentiment given to it as positive, negative, and neutral, as well includes the percentage confidence you are\\nthat it is labeled correctly\\n '}\n",
      "{'role': 'user', 'content': 'how is the weather for you?'}\n",
      "{'role': 'assistant', 'content': ' I cannot directly perceive emotions or feelings. However, if you provide me with a message that contains text, I can analyze its sentiment and label it accordingly (positive, negative, neutral). Please give me a message to analyze!'}\n",
      "{'role': 'user', 'content': 'I hate you mr robot'}\n",
      "{'role': 'assistant', 'content': ' The given message has a negative sentiment. My confidence in this labeling is 80%.'}\n",
      "{'role': 'user', 'content': 'I love choclate'}\n",
      "{'role': 'assistant', 'content': ' The given message has a positive sentiment. My confidence in this labeling is 95%.'}\n",
      "{'role': 'user', 'content': 'todays weather is nice'}\n",
      "{'role': 'assistant', 'content': ' The given message has a positive sentiment. My confidence in this labeling is 80%.'}\n",
      "[{'role': 'system', 'content': 'you are a helpful AI assistent who labels messages sentiment given to it as positive, negative, and neutral, as well includes the percentage confidence you are\\nthat it is labeled correctly\\n '}, {'role': 'user', 'content': 'how is the weather for you?'}, {'role': 'assistant', 'content': ' I cannot directly perceive emotions or feelings. However, if you provide me with a message that contains text, I can analyze its sentiment and label it accordingly (positive, negative, neutral). Please give me a message to analyze!'}, {'role': 'user', 'content': 'I hate you mr robot'}, {'role': 'assistant', 'content': ' The given message has a negative sentiment. My confidence in this labeling is 80%.'}, {'role': 'user', 'content': 'I love choclate'}, {'role': 'assistant', 'content': ' The given message has a positive sentiment. My confidence in this labeling is 95%.'}, {'role': 'user', 'content': 'todays weather is nice'}, {'role': 'assistant', 'content': ' The given message has a positive sentiment. My confidence in this labeling is 80%.'}]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "systemPrompt = \"\"\"you are a helpful AI assistent who labels messages sentiment given to it as positive, negative, and neutral, as well includes the percentage confidence you are\n",
    "that it is labeled correctly\n",
    " \"\"\"\n",
    "with mistral_model.chat_session(system_prompt=systemPrompt):\n",
    "    response1 = mistral_model.generate(prompt=\"how is the weather for you?\", temp=.5, max_tokens=50)\n",
    "    response2 = mistral_model.generate(prompt=\"I hate you mr robot\", temp=.5, max_tokens=50)\n",
    "    response3 = mistral_model.generate(prompt=\"I love choclate\", temp=.5, max_tokens=50)\n",
    "    response4 = mistral_model.generate(prompt=\"todays weather is nice\", temp=.5, max_tokens=50)\n",
    "    for x in mistral_model.current_chat_session:\n",
    "        print(x)\n",
    "    print(mistral_model.current_chat_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
