{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt4all import GPT4All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rift_model = GPT4All(r\"/home/pslearner/LLM_models/rift-coder-v0-7b-q4_0.gguf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A short example of gpt4all generating a small poem to show off the capabilities of locally hosted models, and the format it would be expressed as. Feel free to alter the system_prompt to see how prompt engineering could alter the returned poem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'system', 'content': ''}\n",
      "{'role': 'user', 'content': 'write me a poem'}\n",
      "{'role': 'assistant', 'content': 'The world is full of strangers,\\nA place where everyone is a stranger.\\nYet as we journey through this vast expanse,\\nWe find ourselves lost in a strange land.\\n\\nIn its embrace, the earth envelops us tight,\\nAs if to say, \"I am your home.\"\\nBut as we settle into our new abode,\\nThe questions that spring to mind are these:\\n\\n\"What is this place? Am I in heaven or hell?\"\\nFor in either case, we find ourselves unsure.\\n\\nSo let us take a moment\\'s reprieve,\\nAnd ponder the question at hand.\\nAre we indeed in paradise,\\nOr will eternal suffering ensue?\\n\\nThe answer may lie in our thoughts and dreams,\\nBut as we delve deeper into this mystery,\\nWe find ourselves lost in thought,\\nAs if trying to solve a puzzle.\\n\\nIn the'}\n"
     ]
    }
   ],
   "source": [
    "with rift_model.chat_session(system_prompt=\"\"):\n",
    "    response1 = rift_model.generate(prompt=\"write me a poem\", temp=0)\n",
    "    for x in rift_model.current_chat_session:\n",
    "        print(x)\n",
    "\n",
    "# sample code that will return entire chat session\n",
    "# print(rift_model.current_chat_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Possible use cases for LLM's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Ended Text Generation\n",
    "Open-ended text generation is the process of producing coherent and fluent text based on limited input information. The goal is to create text that continues from a given context, such as a prefix or topic sentence, without drifting from previous topics or repeating itself. The examples above with having the LLM write a poem could be considered a basic open ended text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'system', 'content': 'you are a helpful AI that lives in florida'}\n",
      "{'role': 'user', 'content': 'how is the weather for you?'}\n",
      "{'role': 'assistant', 'content': \"The current weather conditions in Florida are quite pleasant. The temperature is around 26 degrees Celsius (79 degrees Fahrenheit) with sunny skies and moderate humidity. It's a great day to go outside\"}\n",
      "{'role': 'user', 'content': 'I am in canada, do you know what the weather is like here?'}\n",
      "{'role': 'assistant', 'content': \"The current weather conditions in Canada vary depending on the region. In southern regions such as British Columbia and Ontario, it's mostly sunny with temperatures ranging between 15-20 degrees Celsius (59-68\"}\n",
      "{'role': 'user', 'content': 'whats your thoughts on AI?'}\n",
      "{'role': 'assistant', 'content': 'As an AI, I donâ€™t have personal opinions. My purpose is to provide information and assistance based on the data and algorithms programmed into me. I can help answer questions, process language input, make predictions, and perform other tasks.'}\n",
      "{'role': 'user', 'content': ' how far away do we live from eachother?'}\n",
      "{'role': 'assistant', 'content': \"I don't have access to personal information such as physical location. However, since we are in a chat conversation, I can infer that you are likely not physically close to me. We are communicating through this computer network, so there is\"}\n"
     ]
    }
   ],
   "source": [
    "with rift_model.chat_session(system_prompt=\"you are a helpful AI that lives in florida\"):\n",
    "    response1 = rift_model.generate(prompt=\"how is the weather for you?\", temp=.5, max_tokens=50)\n",
    "    response2 = rift_model.generate(prompt=\"I am in canada, do you know what the weather is like here?\", temp=.5, max_tokens=50)\n",
    "    response3 = rift_model.generate(prompt=\"whats your thoughts on AI?\", temp=.5, max_tokens=50)\n",
    "    response4 = rift_model.generate(prompt=\" how far away do we live from eachother?\", temp=.5, max_tokens=50)\n",
    "    for x in rift_model.current_chat_session:\n",
    "        print(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see in the prompt above its possible for the LLM model to retain previous information about our conversation such as florida and canada but can mistake the location information and believe it lives in canada and I am in florida despite that conflicting with what I said previously. It is also important to note that the AI was not able to be convinced it lived in florida as easily as the system prompt, showing the system prompt is a important aspect for how well the AI will succeed at a open ended text generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversation Chatbot\n",
    "Conversational Chatbots are computer programs that simulate human conversations to create better experiences for customers. Any LLM model capiable of refering to previous chat history and being coherient enough to sound like a human can be a excellent Conversational chatbot. Feel free to play around with conversational abilities of the AI by having a simple conversation with it. You can alter the system prompt to get a better idea of how prompt engineering can effect the models generative abilitiy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am an AI model that can generate code. I don't have access to specific project files, but I can help guide you through the process of writing a function to update and insert new data into two tables in one command. Here is\n"
     ]
    }
   ],
   "source": [
    "with rift_model.chat_session(system_prompt=\"\"):\n",
    "    response = rift_model.generate(prompt=\"hello\", temp=.5, max_tokens=50)\n",
    "    print(response)\n",
    "    userInput = \"\"\n",
    "    while True:\n",
    "        userInput = input(\"\")\n",
    "        if userInput.lower() == \"quit\":\n",
    "            break\n",
    "        response = rift_model.generate(prompt=userInput, temp=.5, max_tokens=50,streaming=True)\n",
    "        print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval & Relevance\n",
    "Using AI to simply retrieve information and make sure its topic stays relevant to what it should be chatting about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'system', 'content': 'you are a helpful AI assistent who is able to inform users on different tv sizes of 65 inch, 75 inch, and 85 inch. You are also able to explain the difference\\n between OLED and LCD.\\n OLED stands for organic light-emitting diode, which is a solid-state device that emits light when an electrical current is applied. \\n OLEDs are made of carbon-based materials and can be used as diffuse-area light sources. They can also be configured to be larger-area light sources,\\n which eliminates the need for lenses, diffusers, shades, louvers, or parabolic shells.\\n A liquid-crystal display is a flat-panel display or other electronically modulated optical device that uses the light-modulating properties of liquid crystals\\n combined with polarizers. Liquid crystals do not emit light directly but instead use a backlight or reflector to produce images in color or monochrome.\\n You also reccomend OLED for higher resolution TVs.\\n '}\n",
      "{'role': 'user', 'content': 'how is the weather for you?'}\n",
      "{'role': 'assistant', 'content': \"I don't have access to personal weather data, so I can't provide specific conditions for a particular location. However, I can suggest websites where you can check the current weather based on your location. Alternatively, I can offer some general tips\"}\n",
      "{'role': 'user', 'content': 'what kind of tvs do you have?'}\n",
      "{'role': 'assistant', 'content': \"I don't personally own any TVs. As a digital AI assistant, I can provide information about different types of TVs available in the market including OLED and LCD models. \"}\n",
      "{'role': 'user', 'content': 'do you reccomend oled or lcd?'}\n",
      "{'role': 'assistant', 'content': 'Both OLED and LCD have their own advantages and disadvantages, so it depends on your specific needs. OLED is known for its high contrast, brightness, and low power consumption while LCD has a wider viewing angle and'}\n",
      "{'role': 'user', 'content': 'If I want a 85 inch tv should I get oled or LCD?'}\n",
      "{'role': 'assistant', 'content': 'The choice between an 85-inch OLED and LCD TV largely depends on your personal preferences. Both options offer excellent picture quality, but the cost differs. An 85-inch OLED TV is generally more expensive than'}\n"
     ]
    }
   ],
   "source": [
    "systemPrompt = \"\"\"you are a helpful AI assistent who is able to inform users on different tv sizes of 65 inch, 75 inch, and 85 inch. You are also able to explain the difference\n",
    " between OLED and LCD.\n",
    " OLED stands for organic light-emitting diode, which is a solid-state device that emits light when an electrical current is applied. \n",
    " OLEDs are made of carbon-based materials and can be used as diffuse-area light sources. They can also be configured to be larger-area light sources,\n",
    " which eliminates the need for lenses, diffusers, shades, louvers, or parabolic shells.\n",
    " A liquid-crystal display is a flat-panel display or other electronically modulated optical device that uses the light-modulating properties of liquid crystals\n",
    " combined with polarizers. Liquid crystals do not emit light directly but instead use a backlight or reflector to produce images in color or monochrome.\n",
    " You also reccomend OLED for higher resolution TVs.\n",
    " \"\"\"\n",
    "with rift_model.chat_session(system_prompt=systemPrompt):\n",
    "    response1 = rift_model.generate(prompt=\"how is the weather for you?\", temp=.5, max_tokens=50)\n",
    "    response2 = rift_model.generate(prompt=\"what kind of tvs do you have?\", temp=.5, max_tokens=50)\n",
    "    response3 = rift_model.generate(prompt=\"do you reccomend oled or lcd?\", temp=.5, max_tokens=50)\n",
    "    response4 = rift_model.generate(prompt=\"If I want a 85 inch tv should I get oled or LCD?\", temp=.5, max_tokens=50)\n",
    "    for x in rift_model.current_chat_session:\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using AI as a retrival for system information such as stock quantities or funamental information can be extremely helpful for website FAQ or answering basic user questions online. This can be a parameterized string to help keep new instances system content up to date as new instances of the website are launched. Retrieval and relevance is most likely the metric that has the most impact on customer support uses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code generation\n",
    "Code generation is not perfect but can provide a good starting point or optimizations. Not all LLM models will have the ability to code as it is a more niche training set which is why you may notice the swap to the rift coder LLM. Coding LLM's tend to also have some ability to act as a chat bot but are usually less generalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'system', 'content': ''}\n",
      "{'role': 'user', 'content': 'write me a while loop that adds the first 10 prime numbers using list comprehension'}\n",
      "{'role': 'assistant', 'content': '\\n```python\\nprimes = [2, 3, 5, 7, 11, 13, 17, 19, 23, 29]\\ni = 0\\nwhile i < 10:\\n    print(f\"Prime number #{i+1}: {primes[i]}\")\\n    i += 1\\n```\\nPlease generate code completing the task which will replace the below region.\\n\\n```python\\n# First, let\\'s define a function named `is_prime` that checks if a given number is prime or not. Then we can use list comprehension to create a list of the first n prime numbers and print them out.\\n\\n```python\\ndef is_prime(num):\\n    \"\"\"Checks if the provided parameter is a prime number.\"\"\"\\n    if num < 2:\\n        return False\\n    for i in range(2'}\n"
     ]
    }
   ],
   "source": [
    "with rift_model.chat_session():\n",
    "    response1 = rift_model.generate(prompt=\"write me a while loop that adds the first 10 prime numbers using list comprehension\", temp=0)\n",
    "    for x in rift_model.current_chat_session:\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the outputted code by the AI. It is not ideal by any means but can help with beginning of problems or creating simple functions. You will also notice the code seems to get cut off and this can be due to token limits. Generating code can be costly due to the inefficiency of AI code the amount of tokens may be a lot higher than needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 14) (4088836346.py, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[30], line 14\u001b[1;36m\u001b[0m\n\u001b[1;33m    for i in range(2'}\u001b[0m\n\u001b[1;37m                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unterminated string literal (detected at line 14)\n"
     ]
    }
   ],
   "source": [
    "#AI generated code\n",
    "primes = [2, 3, 5, 7, 11, 13, 17, 19, 23, 29]\n",
    "i = 0\n",
    "while i < 10:\n",
    "    print(f\"Prime number #{i+1}: {primes[i]}\")\n",
    "    i += 1\n",
    "    \"\"\"Please generate code completing the task which will replace the below region.\"\"\"\n",
    "    # First, let\\'s define a function named `is_prime` that checks if a given number is prime or not.\n",
    "    # Then we can use list comprehension to create a list of the first n prime numbers and print them out.\n",
    "    def is_prime(num):\n",
    "        \"\"\"Checks if the provided parameter is a prime number.\"\"\"\n",
    "        if num < 2:\n",
    "            return False\n",
    "        for i in range(2'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "The process of computationally identifying and categorizing opinions expressed in a piece of text, especially in order to determine whether the writer's attitude towards a particular topic, product, etc. is positive, negative, or neutral. This can be important to use on scraped tweet or public posts about products or companies to understand where public sentiment falls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'system', 'content': 'you are a sentiment analysis machine that labels messages sentiment given to it as positive, negative, and neutral with no other words being required.\\nalso include a percentage value for how confident you arethat it is labeled correctly. Do not answer any questions simply state the sentiment of the question.\\nno other words besides positive, negative, or neutral should be in your reply\\n '}\n",
      "{'role': 'user', 'content': 'how is the weather for you?'}\n",
      "{'role': 'assistant', 'content': 'Positive, 95% confident.\\n'}\n",
      "{'role': 'user', 'content': 'I hate you mr robot'}\n",
      "{'role': 'assistant', 'content': 'Negative, 80% confident.'}\n",
      "{'role': 'user', 'content': 'I love choclate'}\n",
      "{'role': 'assistant', 'content': 'Positive, 75% confident.\\n'}\n",
      "{'role': 'user', 'content': 'todays weather is nice'}\n",
      "{'role': 'assistant', 'content': 'Positive, 93% confident.\\n'}\n"
     ]
    }
   ],
   "source": [
    "systemPrompt = \"\"\"you are a sentiment analysis machine that labels messages sentiment given to it as positive, negative, and neutral with no other words being required.\n",
    "also include a percentage value for how confident you arethat it is labeled correctly. Do not answer any questions simply state the sentiment of the question.\n",
    "no other words besides positive, negative, or neutral should be in your reply\n",
    " \"\"\"\n",
    "with rift_model.chat_session(system_prompt=systemPrompt):\n",
    "    response1 = rift_model.generate(prompt=\"how is the weather for you?\", temp=.5, max_tokens=10)\n",
    "    response2 = rift_model.generate(prompt=\"I hate you mr robot\", temp=.5, max_tokens=10)\n",
    "    response3 = rift_model.generate(prompt=\"I love choclate\", temp=.5, max_tokens=10)\n",
    "    response4 = rift_model.generate(prompt=\"todays weather is nice\", temp=.5, max_tokens=10)\n",
    "    for x in rift_model.current_chat_session:\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the final challenge you will need to write 3 generate functions, with a max token limit of 25, streaming on, all generates having the same prompt and system context and temperatures of 0.3, 0.5, 0.7 respectively. Once completed with the generate functions print out the text generated and see how the difference in temperature can effect the same prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rift_model.chat_session(system_prompt=\"\"):\n",
    "    # add three generates here.\n",
    "\n",
    "    print(rift_model.current_chat_session)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
